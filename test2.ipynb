{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract brand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rway_img_df_ok = pd.read_csv(\"collection_images_download.csv\")\n",
    "rway_img_df = rway_img_df_ok.copy()\n",
    "\n",
    "# EXTRACT BRAND FROM IMAGE NAME\n",
    "# Creazione delle nuove colonne\n",
    "rway_img_df['brand'] = rway_img_df['image_name'].str.lower()\n",
    "rway_img_df['brand'] = rway_img_df['brand'].str.split('couture').str[0]\n",
    "rway_img_df['brand'] = rway_img_df['brand'].str.split('ready-to-wear').str[0]\n",
    "rway_img_df['brand'] = rway_img_df['brand'].str.split('menswear').str[0]\n",
    "rway_img_df['brand'] = rway_img_df['brand'].str.replace(\"-\", \" \")\n",
    "rway_img_df['brand'] = rway_img_df['brand'].str.strip()\n",
    "\n",
    "# subset rway_img_df where brand cointains versage,hermes\n",
    "brand_keywords = ['rick owens', 'louis vuitton', 'versace', 'giorgio armani', 'hermes', 'dries van noten', 'chanel']\n",
    "data = rway_img_df[rway_img_df['brand'].isin(brand_keywords)]\n",
    "\n",
    "# rename brand in target column\n",
    "data['target'] = data['brand']\n",
    "# factorized 'brand' using pandas\n",
    "data['target'] = pd.factorize(data['target'])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the value counts of each brand in the 'brand' column\n",
    "brand_counts = rway_img_df['brand'].value_counts()\n",
    "\n",
    "# Print the brand names with count greater than 800\n",
    "for brand, count in brand_counts.items():\n",
    "    if count > 800:\n",
    "        print(brand, count)\n",
    "\n",
    "#! here the list of brand to predict without any class imbalance\n",
    "#* ['rick owens', 'yohji yamamoto', 'louis vuitton', 'versace', 'giorgio armani', 'hermes', 'dries van noten', 'chanel']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, utils\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data\n",
    "labels=[]\n",
    "images=[]\n",
    "for i in range(data.shape[0]):\n",
    "    images.append('./imgs/images/images/'  + data['image_path'].iloc[i])\n",
    "    labels.append(data['target'].iloc[i])\n",
    "df=pd.DataFrame(images)\n",
    "df.columns=['images']\n",
    "df['target']=labels\n",
    "\n",
    "# Split train into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['images'],df['target'], test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomRotation(10),      # rotate +/- 10 degrees\n",
    "        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n",
    "        transforms.Resize(224),             # resize shortest side to 224 pixels\n",
    "        transforms.CenterCrop(224),         # crop longest side to 224 pixels at center\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,data_paths,labels,transform=None,mode='train'):\n",
    "         self.data=data_paths\n",
    "         self.labels=labels\n",
    "         self.transform=transform\n",
    "         self.mode=mode\n",
    "    def __len__(self):\n",
    "       return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_name = self.data[idx]\n",
    "        img = cv2.imread(img_name)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img=Image.fromarray(img)\n",
    "        if self.transform is not None:\n",
    "          img = self.transform(img)\n",
    "        img=img.to('mps')\n",
    "        \n",
    "        labels = torch.tensor(self.labels[idx]).cpu()\n",
    "\n",
    "        return img, labels\n",
    "            \n",
    "train_dataset=ImageDataset(data_paths=X_train.values,labels=y_train.values,transform=train_transform)\n",
    "val_dataset=ImageDataset(data_paths=X_val.values,labels=y_val.values,transform=val_transform)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size=100,shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=50,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - VGG16 with Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_features_l, in_features_g, attn_features, up_factor, normalize_attn=True):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.up_factor = up_factor\n",
    "        self.normalize_attn = normalize_attn\n",
    "        self.W_l = nn.Conv2d(in_channels=in_features_l, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n",
    "        self.W_g = nn.Conv2d(in_channels=in_features_g, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n",
    "        self.phi = nn.Conv2d(in_channels=attn_features, out_channels=1, kernel_size=1, padding=0, bias=True)\n",
    "    def forward(self, l, g):\n",
    "        N, C, W, H = l.size()\n",
    "        l_ = self.W_l(l)\n",
    "        g_ = self.W_g(g)\n",
    "        if self.up_factor > 1:\n",
    "            g_ = F.interpolate(g_, scale_factor=self.up_factor, mode='bilinear', align_corners=False)\n",
    "        c = self.phi(F.relu(l_ + g_)) # batch_sizex1xWxH\n",
    "        \n",
    "        # compute attn map\n",
    "        if self.normalize_attn:\n",
    "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,W,H)\n",
    "        else:\n",
    "            a = torch.sigmoid(c)\n",
    "        # re-weight the local feature\n",
    "        f = torch.mul(a.expand_as(l), l) # batch_sizexCxWxH\n",
    "        if self.normalize_attn:\n",
    "            output = f.view(N,C,-1).sum(dim=2) # weighted sum\n",
    "        else:\n",
    "            output = F.adaptive_avg_pool2d(f, (1,1)).view(N,C) # global average pooling\n",
    "        return a, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnVGG(nn.Module):\n",
    "    def __init__(self, num_classes, normalize_attn=False, dropout=None):\n",
    "        super(AttnVGG, self).__init__()\n",
    "        net = models.vgg16_bn(pretrained=True)\n",
    "        self.conv_block1 = nn.Sequential(*list(net.features.children())[0:6])\n",
    "        self.conv_block2 = nn.Sequential(*list(net.features.children())[7:13])\n",
    "        self.conv_block3 = nn.Sequential(*list(net.features.children())[14:23])\n",
    "        self.conv_block4 = nn.Sequential(*list(net.features.children())[24:33])\n",
    "        self.conv_block5 = nn.Sequential(*list(net.features.children())[34:43])\n",
    "        self.pool = nn.AvgPool2d(7, stride=1)\n",
    "        self.dpt = None\n",
    "        if dropout is not None:\n",
    "            self.dpt = nn.Dropout(dropout)\n",
    "        self.cls = nn.Linear(in_features=512+512+256, out_features=num_classes, bias=True)\n",
    "        \n",
    "       # initialize the attention blocks defined above\n",
    "        self.attn1 = AttentionBlock(256, 512, 256, 4, normalize_attn=normalize_attn)\n",
    "        self.attn2 = AttentionBlock(512, 512, 256, 2, normalize_attn=normalize_attn)\n",
    "        \n",
    "       \n",
    "        self.reset_parameters(self.cls)\n",
    "        self.reset_parameters(self.attn1)\n",
    "        self.reset_parameters(self.attn2)\n",
    "    def reset_parameters(self, module):\n",
    "        for m in module.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1.)\n",
    "                nn.init.constant_(m.bias, 0.)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0., 0.01)\n",
    "                nn.init.constant_(m.bias, 0.)\n",
    "    def forward(self, x):\n",
    "        block1 = self.conv_block1(x)       # /1\n",
    "        pool1 = F.max_pool2d(block1, 2, 2) # /2\n",
    "        block2 = self.conv_block2(pool1)   # /2\n",
    "        pool2 = F.max_pool2d(block2, 2, 2) # /4\n",
    "        block3 = self.conv_block3(pool2)   # /4\n",
    "        pool3 = F.max_pool2d(block3, 2, 2) # /8\n",
    "        block4 = self.conv_block4(pool3)   # /8\n",
    "        pool4 = F.max_pool2d(block4, 2, 2) # /16\n",
    "        block5 = self.conv_block5(pool4)   # /16\n",
    "        pool5 = F.max_pool2d(block5, 2, 2) # /32\n",
    "        N, __, __, __ = pool5.size()\n",
    "        \n",
    "        g = self.pool(pool5).view(N,512)\n",
    "        a1, g1 = self.attn1(pool3, pool5)\n",
    "        a2, g2 = self.attn2(pool4, pool5)\n",
    "        g_hat = torch.cat((g,g1,g2), dim=1) # batch_size x C\n",
    "        if self.dpt is not None:\n",
    "            g_hat = self.dpt(g_hat)\n",
    "        out = self.cls(g_hat)\n",
    "\n",
    "        return [out, a1, a2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttnVGG(num_classes=1, normalize_attn=True)\n",
    "model = model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "        \n",
    "criterion = FocalLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 2\n",
    "loss_epoch_test = []\n",
    "train_losses = []\n",
    "train_auc=[]\n",
    "val_auc=[]\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    train_preds=[]\n",
    "    train_targets=[]\n",
    "    auc_train=[]\n",
    "    loss_epoch_train=[]\n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in tqdm(enumerate(train_loader),total=len(train_loader)):\n",
    "        \n",
    "        b+=1\n",
    "        y_pred,_,_=model(X_train)\n",
    "        loss = criterion(torch.sigmoid(y_pred.type(torch.FloatTensor)), y_train.type(torch.FloatTensor))   \n",
    "        loss_epoch_train.append(loss.item())\n",
    "        # For plotting purpose\n",
    "        if (i==1):\n",
    "            if (b==19):\n",
    "                I_train = utils.make_grid(X_train[0:8,:,:,:], nrow=8, normalize=True, scale_each=True)\n",
    "                __, a1, a2 = model(X_train[0:8,:,:,:])\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "                 \n",
    "    try:\n",
    "        auc_train=roc_auc_score(y_train.detach().to(device).numpy(),torch.sigmoid(y_pred).detach().to(device).numpy())\n",
    "    except:\n",
    "        auc_train=0\n",
    "    train_losses.append(np.mean(loss_epoch_train))\n",
    "    train_auc.append(auc_train)\n",
    "    print(f'epoch: {i:2}   loss: {np.mean(loss_epoch_train):10.8f} AUC  : {auc_train:10.8f} ')\n",
    "    # Run the testing batches\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(val_loader):\n",
    "            \n",
    "            y_val,_,_ = model(X_test)\n",
    "            loss = criterion(torch.sigmoid(y_val.type(torch.FloatTensor)), y_test.type(torch.FloatTensor))\n",
    "            loss_epoch_test.append(loss.item())\n",
    "    val_auc.append(auc_val)\n",
    "    print(f'Epoch: {i} Val Loss: {np.mean(loss_epoch_test):10.8f} AUC: {auc_val:10.8f} ')\n",
    "    \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
